# 警告：Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
    在代码中加入如下代码，忽略警告
    import os

    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

# 五分钟带你入门TensorFlow

https://www.jianshu.com/p/2ea7a0632239

## TensorFlow的基础运算

在搞神经网络之前，先让我们把TensorFlow的基本运算，也就是加减乘除搞清楚。

首先，TensorFlow有几个概念需要进行明确：

### 1 图（Graph）：用来表示计算任务，也就我们要做的一些操作。

### 2 会话（Session）：建立会话，此时会生成一张空图；在会话中添加节点和边，形成一张图，一个会话可以有多个图，通过执行这些图得到结果。如果把每个图看做一个车床，那会话就是一个车间，里面有若干个车床，用来把数据生产成结果。

### 3 Tensor：用来表示数据，是我们的原料。

### 4 变量（Variable）：用来记录一些数据和状态，是我们的容器。

### 5 feed和fetch：可以为任意的操作(arbitrary operation) 赋值或者从其中获取数据。相当于一些铲子，可以操作数据。

形象的比喻是：把会话看做车间，图看做车床，里面用Tensor做原料，变量做容器，feed和fetch做铲子，把数据加工成我们的结果。



# 用TensorFlow玩转Kaggle的“手写识别

参考资料
http://wiki.jikexueyuan.com/project/tensorflow-zh/tutorials/mnist_beginners.html

数据集
http://yann.lecun.com/exdb/mnist/

本例的数据集下载 :
 https://www.kaggle.com/c/digit-recognizer/data

## 手写图片识别的实现，分为三步：

    一、数据的准备

    二、模型的设计

    三、代码实现

### 一、数据的准备

Kaggle里包含了42000份训练数据和28000份测试数据（和谷歌准备的MNIST数据，在数量上有所不同）。
训练和测试数据的下载地址可以百度也可以点这里。下载下来是两个CVS文件。


Kaggle的数据都是表格形式的，和MNIST给的图片不一样。但实际上只是对图片的信息进行了处理，把一个28*28的图片信息，
变成了28*28=784的一行数据。

28*28 = 784，也就是说，这个二维数组可以转为一个784个数字组成的一维数组。

扁平化会丢失图片的二维结构信息，好的图形结构算法都会利用二维结构信息，但是为了简化过程便于理解，这里先使用这种一维结构来进行分析。

这样，上面的训练数据和测试数据，都可以分别转化为[42000,769]和[28000,768]的数组。

为什么训练数据会多一列呢？因为有一列存的是这个图片的结果。好我们继续来看图片：

### 二、模型的设计

不想看理论的可以跳过这一步，直接进入代码环节
这个模型，组成是这样的：

    1）使用一个最简单的单层的神经网络进行学习

    2）用SoftMax来做为激活函数

    3）用交叉熵来做损失函数

    4）用梯度下降来做优化方式

### 这里有几个新的名词，神经网络、激活函数、SoftMax、损失函数、交叉熵、梯度下降，我们挨个解释一下。

    神经网络：由很多个神经元组成，每个神经元接收很多个输入：[X1,X2....Xn]，加权相加然后加上偏移量后，看是不是超过了某个阀值，
    超过了发出1，没超过发出0。

    激活函数：每个神经元，在通过一系列计算后，得到了一个数值，怎么来判断应该输出什么呢？激活函数就是解决这个问题，你把值给我，我来判断怎么输出。所以一个神经网络，激活函数是非常重要的。

    想要成为激活函数，你得有两把刷子啊。这两把刷子是：一是你得处处可微，可微分才能求导，求极值。二是要非线性的，因为线性模型的表达能力不够。

    目前主流的几个激活函数是：sigmoid,tanh,ReLU。

        sigmoid：采用S形函数，取值范围[0,1]

        tanh：双切正切函数，取值范围[-1,1]

        ReLU：简单而粗暴，大于0的留下，否则一律为0。

    SoftMax：我们知道max(A,B)，�是指A和B里哪个大就取哪个值，但我们有时候希望比较小的那个也有一定概率取到，怎么办呢？我们就按照两个值的大小，计算出概率，按照这个概率来取A或者B。比如A=9，B=1,那取A的概率是90%，取B的概率是10%。
        这个看起来比max(A,B)这样粗暴的方式柔和一些，所以叫SoftMax（名字解释纯属个人瞎掰😑大家能理解概念就好）

    损失函数：损失函数是模型对数据拟合程度的反映，拟合得越好损失应该越小，拟合越差损失应该越大，然后我们根据损失函数的结果对模型进行调整。

    交叉熵：这个概念要解释的简单，那就不准确，如果要准确，那可能一千字都打不住。这里说一个简单但不一定准确的解释吧。

        比如，你想把乾坤大挪移练到第七层大圆满，你现在是第五层，那你还差两层，这个两层就是你和大圆满之间的距离。交叉熵通俗的讲就是现在的训练程度和圆满之间的距离，我们希望距离越小越好，所以交叉熵可以作为一个损失函数，来衡量和目标之间的距离。

    梯度下降：这个概念可以这样理解，我们要解决的问题是一座山，答案在山底，我们从山顶到山底的过程就是解决问题的过程。

        在山顶，想找到最快的下山的路。这个时候，我们的做法是什么呢？在每次选择道路的时候，选最陡的那条路。梯度是改变率或者斜度的另一个称呼，用数学的语言解释是导数。对于求损失函数最小值这样的问题，朝着梯度下降的方向走，就能找到最优值了。

三、代码实现

1 载入数据，并对数据进行处理

在写代码的过程中，数据的预处理是最大的一块工作，做一个项目，60%以上的代码在做数据预处理。

这个项目的预处理，分为5步：

    1）把输入和结果分开

    2）对输入进行处理：把一维的输入变成28*28的矩阵

    3）对结果进行处理：把结果进行One-Hot编码

    4）把训练数据划分训练集和验证集

    5）对训练集进行分批
