# [高阶内容](https://morvanzhou.github.io/tutorials/machine-learning/tensorflow/5-01-classifier/) #


## Classification 分类学习 ##

### 学习资料: ###

- [相关代码](https://github.com/MorvanZhou/tutorials/tree/master/tensorflowTUT/tf16_classification)
- 为 TF 2017 [打造的新版可视化教学代码](https://github.com/MorvanZhou/Tensorflow-Tutorial)
- 
这次我们会介绍如何使用TensorFlow解决Classification（分类）问题。 之前的视频讲解的是Regression (回归)问题。 分类和回归的区别在于输出变量的类型上。 通俗理解定量输出是回归，或者说是连续变量预测； 定性输出是分类，或者说是离散变量预测。如预测房价这是一个回归任务； 把东西分成几类, 比如猫狗猪牛，就是一个分类任务。


### MNIST 数据  ###
首先准备数据（MNIST库）

	from tensorflow.examples.tutorials.mnist import input_data
	mnist = input_data.read_data_sets('MNIST_data', one_hot=True)

MNIST库是手写体数字库，差不多是这样子的

![](https://morvanzhou.github.io/static/results/tensorflow/5_01_1.png)

数据中包含55000张训练图片，每张图片的分辨率是28×28，所以我们的训练网络输入应该是28×28=784个像素数据。

### 搭建网络  ###
	xs = tf.placeholder(tf.float32, [None, 784]) # 28x28
每张图片都表示一个数字，所以我们的输出是数字0到9，共10类。

	ys = tf.placeholder(tf.float32, [None, 10])
调用add_layer函数搭建一个最简单的训练网络结构，只有输入层和输出层。

	prediction = add_layer(xs, 784, 10, activation_function=tf.nn.softmax)
其中输入数据是784个特征，输出数据是10个特征，激励采用softmax函数，网络结构图是这样子的

![](https://morvanzhou.github.io/static/results/tensorflow/5_01_2.png)


### Cross entropy loss ### 
loss函数（即最优化目标函数）选用交叉熵函数。交叉熵用来衡量预测值和真实值的相似程度，如果完全相同，它们的交叉熵等于零。

	cross_entropy = tf.reduce_mean(-tf.reduce_sum(ys * tf.log(prediction),
	reduction_indices=[1])) # loss
train方法（最优化算法）采用梯度下降法。

	train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)
	sess = tf.Session()
	# tf.initialize_all_variables() 这种写法马上就要被废弃
	# 替换成下面的写法:
	sess.run(tf.global_variables_initializer())

### 训练  ###
现在开始train，每次只取100张图片，免得数据太多训练太慢。

	batch_xs, batch_ys = mnist.train.next_batch(100)
	sess.run(train_step, feed_dict={xs: batch_xs, ys: batch_ys})
每训练50次输出一下预测精度

	if i % 50 == 0:
	        print(compute_accuracy(
	            mnist.test.images, mnist.test.labels))
输出结果如下：

[ Classification 分类学习](https://morvanzhou.github.io/static/results/tensorflow/5_01_3.png)

有没有很惊讶啊，如此简单的神经网络结构竟然可以达到这样的图像识别精度，其实稍作改动后，识别的精度将大幅提高。 请关注后续课程哦。


## 什么是过拟合 (Overfitting) ##

### 学习资料: ###

- Tensorflow: dropout [教程](https://morvanzhou.github.io/tutorials/machine-learning/tensorflow/5-02-dropout/)
- PyTorch: dropout [教程](https://morvanzhou.github.io/tutorials/machine-learning/torch/5-03-dropout/)
- Theano: l1 l2 regularization [教程](https://morvanzhou.github.io/tutorials/machine-learning/theano/3-5-regularization/)
今天我们会来聊聊机器学习中的过拟合 overfitting 现象, 和解决过拟合的方法.



### 过于自负  ###
![](https://morvanzhou.github.io/static/results/ML-intro/overfitting1.png)

在细说之前, 我们先用实际生活中的一个例子来比喻一下过拟合现象. 说白了, 就是机器学习模型于自信. 已经到了自负的阶段了. 那自负的坏处, 大家也知道, 就是在自己的小圈子里表现非凡, 不过在现实的大圈子里却往往处处碰壁. 所以在这个简介里, 我们把自负和过拟合画上等号.

### 回归分类的过拟合  ###
![](https://morvanzhou.github.io/static/results/ML-intro/overfitting2.png)

机器学习模型的自负又表现在哪些方面呢. 这里是一些数据. 如果要你画一条线来描述这些数据, 大多数人都会这么画. 对, 这条线也是我们希望机器也能学出来的一条用来总结这些数据的线. 这时蓝线与数据的总误差可能是10. 可是有时候, 机器过于纠结这误差值, 他想把误差减到更小, 来完成他对这一批数据的学习使命. 所以, 他学到的可能会变成这样 . 它几乎经过了每一个数据点, 这样, 误差值会更小 . 可是误差越小就真的好吗? 看来我们的模型还是太天真了. 当我拿这个模型运用在现实中的时候, 他的自负就体现出来. 小二, 来一打现实数据 . 这时, 之前误差大的蓝线误差基本保持不变 .误差小的 红线误差值突然飙高 , 自负的红线再也骄傲不起来, 因为他不能成功的表达除了训练数据以外的其他数据. 这就叫做过拟合. Overfitting.

![](https://morvanzhou.github.io/static/results/ML-intro/overfitting3.png)

那么在分类问题当中. 过拟合的分割线可能是这样, 小二, 再上一打数据 . 我们明显看出, 有两个黄色的数据并没有被很好的分隔开来. 这也是过拟合在作怪.好了, 既然我们时不时会遇到过拟合问题, 那解决的方法有那些呢.

### 解决方法  ###
![](https://morvanzhou.github.io/static/results/ML-intro/overfitting4.png)

方法一: 增加数据量, 大部分过拟合产生的原因是因为数据量太少了. 如果我们有成千上万的数据, 红线也会慢慢被拉直, 变得没那么扭曲 . 方法二:

![](https://morvanzhou.github.io/static/results/ML-intro/overfitting5.png)

运用正规化. L1, l2 regularization等等, 这些方法适用于大多数的机器学习, 包括神经网络. 他们的做法大同小异, 我们简化机器学习的关键公式为 y=Wx . W为机器需要学习到的各种参数. 在过拟合中, W 的值往往变化得特别大或特别小. 为了不让W变化太大, 我们在计算误差上做些手脚. 原始的 cost 误差是这样计算, cost = 预测值-真实值的平方. 如果 W 变得太大, 我们就让 cost 也跟着变大, 变成一种惩罚机制. 所以我们把 W 自己考虑进来. 这里 abs 是绝对值. 这一种形式的 正规化, 叫做 l1 正规化. L2 正规化和 l1 类似, 只是绝对值换成了平方. 其他的l3, l4 也都是换成了立方和4次方等等. 形式类似. 用这些方法,我们就能保证让学出来的线条不会过于扭曲.

![](https://morvanzhou.github.io/static/results/ML-intro/overfitting6.png)

还有一种专门用在神经网络的正规化的方法, 叫作 dropout. 在训练的时候, 我们随机忽略掉一些神经元和神经联结 , 是这个神经网络变得”不完整”. 用一个不完整的神经网络训练一次.

到第二次再随机忽略另一些, 变成另一个不完整的神经网络. 有了这些随机 drop 掉的规则, 我们可以想象其实每次训练的时候, 我们都让每一次预测结果都不会依赖于其中某部分特定的神经元. 像l1, l2正规化一样, 过度依赖的 W , 也就是训练参数的数值会很大, l1, l2会惩罚这些大的 参数. Dropout 的做法是从根本上让神经网络没机会过度依赖.


## Dropout 解决 overfitting ##

### 要定  ###
Overfitting 也被称为过度学习，过度拟合。 它是机器学习中常见的问题。 举个Classification（分类）的例子。

![](https://morvanzhou.github.io/static/results/tensorflow/5_02_1.png)

图中黑色曲线是正常模型，绿色曲线就是overfitting模型。尽管绿色曲线很精确的区分了所有的训练数据，但是并没有描述数据的整体特征，对新测试数据的适应性较差。

举个Regression (回归)的例子，

![](https://morvanzhou.github.io/static/results/tensorflow/5_02_2.png)

第三条曲线存在overfitting问题，尽管它经过了所有的训练点，但是不能很好的反应数据的趋势，预测能力严重不足。 TensorFlow提供了强大的dropout方法来解决overfitting问题。

### 建立 dropout 层  ###
本次内容需要使用一下 sklearn 数据库当中的数据, 没有安装 sklearn 的同学可以参考一下这个教程 安装一下. 然后 import 以下模块.

	import tensorflow as tf
	from sklearn.datasets import load_digits
	from sklearn.cross_validation import train_test_split
	from sklearn.preprocessing import LabelBinarizer
	keep_prob = tf.placeholder(tf.float32)
	...
	...
	Wx_plus_b = tf.nn.dropout(Wx_plus_b, keep_prob)
这里的keep_prob是保留概率，即我们要保留的结果所占比例，它作为一个placeholder，在run时传入， 当keep_prob=1的时候，相当于100%保留，也就是dropout没有起作用。 下面我们分析一下程序结构，首先准备数据，

	digits = load_digits()
	X = digits.data
	y = digits.target
	y = LabelBinarizer().fit_transform(y)
	X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3)
其中X_train是训练数据, X_test是测试数据。 然后添加隐含层和输出层

	# add output layer
	l1 = add_layer(xs, 64, 50, 'l1', activation_function=tf.nn.tanh)
	prediction = add_layer(l1, 50, 10, 'l2', activation_function=tf.nn.softmax)
loss函数（即最优化目标函数）选用交叉熵函数。交叉熵用来衡量预测值和真实值的相似程度，如果完全相同，交叉熵就等于零。

	cross_entropy = tf.reduce_mean(-tf.reduce_sum(ys * tf.log(prediction),                                         reduction_indices=[1]))  # loss
train方法（最优化算法）采用梯度下降法。

	train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)


### 训练  ###
最后开始train，总共训练500次。

	sess.run(train_step, feed_dict={xs: X_train, ys: y_train, keep_prob: 0.5})
	#sess.run(train_step, feed_dict={xs: X_train, ys: y_train, keep_prob: 1})
### 可视化结果  ###
训练中keep_prob=1时，就可以暴露出overfitting问题。keep_prob=0.5时，dropout就发挥了作用。 我们可以两种参数分别运行程序，对比一下结果。

当keep_prob=1时，模型对训练数据的适应性优于测试数据，存在overfitting，输出如下： 红线是 train 的误差, 蓝线是 test 的误差.

 ![](https://morvanzhou.github.io/static/results/tensorflow/5_02_3.png)

当keep_prob=0.5时效果好了很多，输出如下：

 ![](https://morvanzhou.github.io/static/results/tensorflow/5_02_4.png)

程序中用到了Tensorboard输出结果，可以参考前面教程:


### 可能会遇到的问题 ### 
由于评论区中讨论了很多这份代码的问题, 我在此说明一下. 因为 Tensorflow 升级改版了, 原本视频中可以执行的代码可能会遇到一些问题. 强烈推荐看看我2017年根据新版本的 Tensorflow 写的升级版, 简化版代码, 比旧版本的更容易懂, 而且可视化效果做得更好. 里面也有 Dropout 这节内容.





