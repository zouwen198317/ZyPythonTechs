信息熵
  一条信息的信息量大小和它的不确定性有直接的关系，要搞清一件非常非常不确定的事情，或者是我们一无所知的事情，需要了解大量信息，信息
  量的度量就等于不确定性的多少
  
-(p1*log p1+p2*log p2 + ... + p32*log p32)

比特bit来衡量信息的多少
变量的不确定性越大，熵就越大

信息获取量(Information Gain)：Gain(A) = Info(D) - Infor_A(D)


![](https://i.imgur.com/ZaGq5uV.png)


## 决策树归纳算法 （ID3） ##
- 树以代表训练样本的单个结点开始（步骤1）。
- 如果样本都在同一个类，则该结点成为树叶，并用该类标号（步骤2 和3）。
- 否则，算法使用称为信息增益的基于熵的度量作为启发信息，选择能够最好地将样本分类的属性（步骤6）。该属性成为该结点的“测试”或“判定”属性（步骤7）。在算法的该版本中，
- 所有的属性都是分类的，即离散值。连续属性必须离散化。
- 对测试属性的每个已知的值，创建一个分枝，并据此划分样本（步骤8-10）。
- 算法使用同样的过程，递归地形成每个划分上的样本判定树。一旦一个属性出现在一个结点上，就不必该结点的任何后代上考虑它（步骤13）。
- 递归划分步骤仅当下列条件之一成立停止：
- (a) 给定结点的所有样本属于同一类（步骤2 和3）。
- (b) 没有剩余属性可以用来进一步划分样本（步骤4）。在此情况下，使用多数表决（步骤5）。
- 这涉及将给定的结点转换成树叶，并用样本中的多数所在的类标记它。替换地，可以存放结
- 点样本的类分布。
- (c) 分枝
- test_attribute = a i 没有样本（步骤11）。在这种情况下，以 samples 中的多数类
- 创建一个树叶（步骤12）
 

[示例地址](http://scikit-learn.org/stable/modules/tree.html)
